# OBSS AI Challenge

This repository contains my solution for the **OBSS AI Image Captioning Challenge**, a multimodal deep learning competition organized as part of the OBSS AI Internship Program.

The task was to generate semantically accurate captions for images using large-scale vision-language models. Final evaluation was based on a custom embedding-based metric called **Fr√©chet GTE Distance (FGD)**.



---

## üìå Project Overview

The objective was to build an end-to-end image captioning system capable of generating contextually relevant descriptions for 21,000+ images sourced from the Open Images dataset.

Unlike traditional lexical metrics (BLEU, ROUGE), this competition used a semantic evaluation metric based on embedding distributions, encouraging models to generate meaningfully aligned captions rather than surface-level text similarity.

---

## üß† Models Explored

During experimentation, the following multimodal architectures were trained and evaluated:

- **BLIP (Bootstrapped Language Image Pretraining)** ‚Äì fine-tuned and selected as final model
- **CLIP-based approaches**
- **LLaVA (Large Language and Vision Assistant)**

Comparative benchmarking was performed to evaluate:

- Caption semantic coherence
- Generalization capability
- Performance under FGD metric

The final submission was generated using a fine-tuned **BLIP** model.

---

## üìä Evaluation Metric ‚Äì Fr√©chet GTE Distance (FGD)

The competition used a modified Fr√©chet distance computed over text embeddings generated by the **GTE-small** model.

Evaluation process:
1. Convert predicted and ground-truth captions into embedding vectors.
2. Compute mean and covariance of each distribution.
3. Calculate Fr√©chet distance between embedding distributions.

Lower FGD scores indicate better semantic alignment.

This required optimizing captions not only for fluency but for embedding-space similarity.

---

## ‚öôÔ∏è Technical Stack

- Python
- PyTorch
- HuggingFace Transformers
- BLIP / CLIP / LLaVA
- Sentence Transformers (GTE-small)
- NumPy / pandas
- Google Colab (GPU training & inference)

---

## üî¨ Methodology

1. **Data Preparation**
   - Structured training dataset from provided CSV annotations
   - Image-caption alignment preprocessing

2. **Model Training & Fine-Tuning**
   - Fine-tuned BLIP for caption generation
   - Hyperparameter tuning for improved semantic alignment
   - Controlled experimentation across architectures

3. **Optimization for FGD**
   - Evaluated embedding-space similarity
   - Improved decoding strategies and generation parameters
   - Iterative leaderboard-based experimentation

4. **Submission Pipeline**
   - Generated reproducible inference workflow
   - Produced submission.csv file matching competition format

---

## üìà Key Takeaways

- Multimodal transformers significantly outperform classical captioning pipelines.
- Embedding-space metrics require optimizing for semantic similarity rather than lexical overlap.
- Fine-tuning BLIP provided the best trade-off between caption quality and metric performance.

---

## üìå About the Competition

The OBSS AI Image Captioning Challenge was part of the OBSS Deep Learning Internship Program, where participants were evaluated based on leaderboard performance and technical submission quality.

