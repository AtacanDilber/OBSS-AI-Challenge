{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === ULTRA-OPTIMIZED BLIP Fine-tuning - Maximum Accuracy & Performance ===\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageEnhance\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from torchvision import transforms\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# === ULTRA Configuration - Maximum Accuracy Focus ===\n",
    "class UltraConfig:\n",
    "    # Paths\n",
    "    TRAIN_CSV_PATH = \"/content/drive/MyDrive/Obss Data/train.csv\"\n",
    "    IMAGE_FOLDER = \"/content/drive/MyDrive/Obss Data/train/train\"\n",
    "    BASE_MODEL = \"Salesforce/blip-image-captioning-base\"\n",
    "    SAVE_PATH = \"/content/drive/MyDrive/Obss Data/blip_finetuned_ULTRA\"\n",
    "    LOG_PATH = \"/content/drive/MyDrive/Obss Data/ultra_training_logs.json\"\n",
    "    \n",
    "    # Model Settings - Optimized for BLIP\n",
    "    IMG_SIZE = 384  # BLIP's optimal resolution\n",
    "    MAX_TEXT_LENGTH = 80  # Slightly increased for complex captions\n",
    "    \n",
    "    # Training Hyperparameters - Fine-tuned for maximum accuracy\n",
    "    BATCH_SIZE = 10  # Slightly smaller for more stable gradients\n",
    "    ACCUMULATION_STEPS = 3  # Effective batch size = 30\n",
    "    LEARNING_RATE = 1.5e-5  # Slightly lower for more careful learning\n",
    "    WEIGHT_DECAY = 0.08  # Increased regularization\n",
    "    NUM_EPOCHS = 20  # More epochs for thorough learning\n",
    "    WARMUP_RATIO = 0.15  # Longer warmup for stability\n",
    "    MAX_GRAD_NORM = 0.8  # Tighter gradient clipping\n",
    "    EARLY_STOPPING_PATIENCE = 6  # More patience for best convergence\n",
    "    SEED = 42\n",
    "    \n",
    "    # Advanced Training Techniques\n",
    "    USE_FOCAL_LOSS = True  # Better handling of hard examples\n",
    "    FOCAL_ALPHA = 0.75\n",
    "    FOCAL_GAMMA = 2.0\n",
    "    USE_LABEL_SMOOTHING = True  # Prevents overconfidence\n",
    "    LABEL_SMOOTHING = 0.1\n",
    "    USE_COSINE_RESTART = True  # Better learning rate schedule\n",
    "    RESTART_FACTOR = 2\n",
    "    \n",
    "    # Quality & Validation\n",
    "    VALIDATION_SPLIT = 0.12  # Smaller validation for more training data\n",
    "    VALIDATION_EVERY_N_STEPS = 500  # Less frequent but thorough validation\n",
    "    SAVE_EVERY_IMPROVEMENT = True\n",
    "    \n",
    "    # Advanced Data Augmentation\n",
    "    USE_ADVANCED_AUGMENTATION = True\n",
    "    AUGMENTATION_STRENGTH = 0.3\n",
    "    USE_CUTOUT = True\n",
    "    USE_MIXUP = False  # Disabled as it can confuse caption generation\n",
    "    CUTOUT_PROB = 0.15\n",
    "\n",
    "config = UltraConfig()\n",
    "\n",
    "# === Enhanced Reproducibility Setup ===\n",
    "def set_ultra_seed(seed_value):\n",
    "    \"\"\"Ultra-comprehensive seed setting for maximum reproducibility.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    \n",
    "    # Additional reproducibility settings\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    print(f\"ðŸŒ± Ultra-seed set to {seed_value} with maximum reproducibility\")\n",
    "\n",
    "# === Advanced Device Setup ===\n",
    "def setup_ultra_device():\n",
    "    \"\"\"Setup device with maximum optimization for training.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        \n",
    "        # Clear and optimize\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Memory optimization\n",
    "        torch.cuda.set_per_process_memory_fraction(0.95)\n",
    "        \n",
    "        # Enable optimizations\n",
    "        try:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        return device\n",
    "    else:\n",
    "        print(\"No GPU detected - training will be very slow\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = setup_ultra_device()\n",
    "set_ultra_seed(config.SEED)\n",
    "\n",
    "# === Advanced Loss Functions ===\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    \"\"\"Focal Loss for handling hard examples better.\"\"\"\n",
    "    def __init__(self, alpha=0.75, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = torch.nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class LabelSmoothingCrossEntropy(torch.nn.Module):\n",
    "    \"\"\"Label smoothing cross entropy loss.\"\"\"\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        log_prob = torch.nn.functional.log_softmax(input, dim=-1)\n",
    "        weight = input.new_ones(input.size()) * self.smoothing / (input.size(-1) - 1.)\n",
    "        weight.scatter_(-1, target.unsqueeze(-1), (1. - self.smoothing))\n",
    "        loss = (-weight * log_prob).sum(dim=-1).mean()\n",
    "        return loss\n",
    "\n",
    "# === Ultra-Advanced Data Augmentation ===\n",
    "class UltraAugmentation:\n",
    "    def __init__(self, img_size=384, strength=0.3, cutout_prob=0.15):\n",
    "        \"\"\"State-of-the-art augmentation for vision-language models.\"\"\"\n",
    "        self.img_size = img_size\n",
    "        self.cutout_prob = cutout_prob\n",
    "        \n",
    "        # Advanced geometric augmentations\n",
    "        self.geometric_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(\n",
    "                img_size, \n",
    "                scale=(0.85, 1.0),\n",
    "                ratio=(0.85, 1.15),\n",
    "                interpolation=transforms.InterpolationMode.BICUBIC\n",
    "            ),\n",
    "            transforms.RandomHorizontalFlip(p=0.35),\n",
    "            transforms.RandomApply([\n",
    "                transforms.RandomRotation(degrees=5, interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "            ], p=0.2),\n",
    "            transforms.RandomApply([\n",
    "                transforms.RandomPerspective(distortion_scale=0.1, p=0.3)\n",
    "            ], p=0.15),\n",
    "        ])\n",
    "        \n",
    "        # Advanced color augmentations\n",
    "        self.color_transform = transforms.Compose([\n",
    "            transforms.ColorJitter(\n",
    "                brightness=0.15 * strength,\n",
    "                contrast=0.15 * strength,\n",
    "                saturation=0.1 * strength,\n",
    "                hue=0.03 * strength\n",
    "            ),\n",
    "            transforms.RandomApply([\n",
    "                transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 0.5))\n",
    "            ], p=0.1),\n",
    "        ])\n",
    "        \n",
    "        # Quality degradation augmentations (simulate real-world conditions)\n",
    "        self.quality_augment_prob = 0.1 * strength\n",
    "        \n",
    "    def enhance_image_quality(self, image):\n",
    "        \"\"\"Occasionally enhance image quality to simulate better lighting.\"\"\"\n",
    "        if random.random() < 0.1:\n",
    "            enhancer = ImageEnhance.Sharpness(image)\n",
    "            image = enhancer.enhance(1.1)\n",
    "            enhancer = ImageEnhance.Contrast(image)\n",
    "            image = enhancer.enhance(1.05)\n",
    "        return image\n",
    "    \n",
    "    def cutout_augment(self, image):\n",
    "        \"\"\"Advanced cutout augmentation.\"\"\"\n",
    "        if random.random() < self.cutout_prob:\n",
    "            # Convert to tensor for cutout\n",
    "            image_tensor = transforms.ToTensor()(image)\n",
    "            h, w = image_tensor.shape[1], image_tensor.shape[2]\n",
    "            \n",
    "            # Random cutout size (5-15% of image)\n",
    "            cutout_size = random.randint(int(0.05 * min(h, w)), int(0.15 * min(h, w)))\n",
    "            y = random.randint(0, h - cutout_size)\n",
    "            x = random.randint(0, w - cutout_size)\n",
    "            \n",
    "            # Fill with random color or mean\n",
    "            if random.random() < 0.5:\n",
    "                image_tensor[:, y:y+cutout_size, x:x+cutout_size] = torch.rand(3, cutout_size, cutout_size)\n",
    "            else:\n",
    "                image_tensor[:, y:y+cutout_size, x:x+cutout_size] = image_tensor.mean()\n",
    "            \n",
    "            return transforms.ToPILImage()(image_tensor)\n",
    "        return image\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        # Apply transformations in order\n",
    "        image = self.enhance_image_quality(image)\n",
    "        image = self.geometric_transform(image)\n",
    "        image = self.color_transform(image)\n",
    "        image = self.cutout_augment(image)\n",
    "        return image\n",
    "\n",
    "# === Smart Dataset with Caption Analysis ===\n",
    "class UltraCaptionDataset(Dataset):\n",
    "    def __init__(self, df, image_folder, processor, is_training=True, caption_stats=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.image_folder = image_folder\n",
    "        self.processor = processor\n",
    "        self.is_training = is_training\n",
    "        self.max_length = config.MAX_TEXT_LENGTH\n",
    "        self.caption_stats = caption_stats\n",
    "        \n",
    "        # Setup augmentation\n",
    "        if is_training and config.USE_ADVANCED_AUGMENTATION:\n",
    "            self.augment = UltraAugmentation(config.IMG_SIZE, config.AUGMENTATION_STRENGTH, config.CUTOUT_PROB)\n",
    "        else:\n",
    "            self.augment = None\n",
    "        \n",
    "        # Analyze captions for better training\n",
    "        if is_training:\n",
    "            self._analyze_captions()\n",
    "        \n",
    "        print(f\"Ultra Dataset: {len(self.df)} samples, Training: {is_training}\")\n",
    "        print(f\"Advanced Augmentation: {'Enabled' if self.augment else 'Disabled'}\")\n",
    "\n",
    "    def _analyze_captions(self):\n",
    "        \"\"\"Analyze caption distribution for better training insights.\"\"\"\n",
    "        caption_lengths = [len(str(cap).split()) for cap in self.df['caption']]\n",
    "        print(f\"Caption Analysis:\")\n",
    "        print(f\"   Avg length: {np.mean(caption_lengths):.1f} words\")\n",
    "        print(f\"   Max length: {max(caption_lengths)} words\")\n",
    "        print(f\"   Min length: {min(caption_lengths)} words\")\n",
    "        \n",
    "        # Find common words\n",
    "        all_words = []\n",
    "        for caption in self.df['caption']:\n",
    "            all_words.extend(str(caption).lower().split())\n",
    "        word_freq = Counter(all_words)\n",
    "        print(f\"   Most common words: {list(word_freq.most_common(10))}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            row = self.df.iloc[idx]\n",
    "            image_path = os.path.join(self.image_folder, f\"{row['image_id']}.jpg\")\n",
    "            \n",
    "            # Load image with better error handling\n",
    "            try:\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                # Verify image is not corrupted\n",
    "                image.verify()\n",
    "                image = Image.open(image_path).convert(\"RGB\")  # Reload after verify\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {image_path}: {e}\")\n",
    "                # Create a more realistic placeholder\n",
    "                image = Image.new('RGB', (config.IMG_SIZE, config.IMG_SIZE), color=(128, 128, 128))\n",
    "            \n",
    "            # Apply augmentation\n",
    "            if self.augment and self.is_training:\n",
    "                image = self.augment(image)\n",
    "            \n",
    "            # Process image\n",
    "            pixel_values = self.processor(\n",
    "                images=image,\n",
    "                return_tensors=\"pt\",\n",
    "                do_rescale=True,\n",
    "                do_normalize=True\n",
    "            ).pixel_values.squeeze(0)\n",
    "            \n",
    "            # Enhanced caption processing\n",
    "            caption = str(row[\"caption\"]).strip()\n",
    "            if len(caption) == 0:\n",
    "                caption = \"an image\"\n",
    "            \n",
    "            # Keep original case but ensure proper formatting\n",
    "            caption = caption.lower().strip()\n",
    "            if not caption.endswith('.'):\n",
    "                caption += '.'\n",
    "            \n",
    "            # Tokenize\n",
    "            text_inputs = self.processor.tokenizer(\n",
    "                caption,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"pixel_values\": pixel_values,\n",
    "                \"input_ids\": text_inputs.input_ids.squeeze(0),\n",
    "                \"attention_mask\": text_inputs.attention_mask.squeeze(0),\n",
    "                \"labels\": text_inputs.input_ids.squeeze(0).clone()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Critical error in dataset[{idx}]: {e}\")\n",
    "            # Robust fallback\n",
    "            dummy_image = Image.new('RGB', (config.IMG_SIZE, config.IMG_SIZE), color=(64, 64, 64))\n",
    "            pixel_values = self.processor(images=dummy_image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "            \n",
    "            emergency_text = self.processor.tokenizer(\n",
    "                \"error sample.\",\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"pixel_values\": pixel_values,\n",
    "                \"input_ids\": emergency_text.input_ids.squeeze(0),\n",
    "                \"attention_mask\": emergency_text.attention_mask.squeeze(0),\n",
    "                \"labels\": emergency_text.input_ids.squeeze(0).clone()\n",
    "            }\n",
    "\n",
    "# === Ultra-Robust Collate Function ===\n",
    "def ultra_collate_fn(batch):\n",
    "    \"\"\"Ultra-robust collate function with advanced error handling.\"\"\"\n",
    "    # Filter out None items\n",
    "    valid_batch = [item for item in batch if item is not None]\n",
    "    \n",
    "    if len(valid_batch) == 0:\n",
    "        print(\"Empty batch encountered\")\n",
    "        return None\n",
    "    \n",
    "    # Handle partial batches\n",
    "    if len(valid_batch) < len(batch):\n",
    "        print(f\"Partial batch: {len(valid_batch)}/{len(batch)} items valid\")\n",
    "    \n",
    "    try:\n",
    "        return {\n",
    "            \"pixel_values\": torch.stack([item[\"pixel_values\"] for item in valid_batch]),\n",
    "            \"input_ids\": torch.stack([item[\"input_ids\"] for item in valid_batch]),\n",
    "            \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in valid_batch]),\n",
    "            \"labels\": torch.stack([item[\"labels\"] for item in valid_batch])\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Collate error: {e}\")\n",
    "        return None\n",
    "\n",
    "# === Load Ultra Model ===\n",
    "def load_ultra_model():\n",
    "    \"\"\"Load model with ultra-optimized settings.\"\"\"\n",
    "    print(f\"Loading {config.BASE_MODEL} with ultra optimization...\")\n",
    "    \n",
    "    processor = BlipProcessor.from_pretrained(config.BASE_MODEL)\n",
    "    \n",
    "    # Load model in FP32 for maximum stability\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\n",
    "        config.BASE_MODEL,\n",
    "        torch_dtype=torch.float32,\n",
    "        low_cpu_mem_usage=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Enable gradient checkpointing for memory efficiency\n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    # Initialize custom loss functions\n",
    "    if config.USE_FOCAL_LOSS:\n",
    "        model.focal_loss = FocalLoss(config.FOCAL_ALPHA, config.FOCAL_GAMMA)\n",
    "    \n",
    "    if config.USE_LABEL_SMOOTHING:\n",
    "        model.label_smoothing_loss = LabelSmoothingCrossEntropy(config.LABEL_SMOOTHING)\n",
    "    \n",
    "    print(f\"Ultra model loaded on {device}\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "# === Ultra Evaluation with Advanced Metrics ===\n",
    "def ultra_evaluation(model, val_loader, device, desc=\"Ultra Validation\"):\n",
    "    \"\"\"Ultra-comprehensive evaluation with advanced metrics.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    batch_count = 0\n",
    "    \n",
    "    print(f\"{desc} - Processing validation...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=desc):\n",
    "            if batch is None:\n",
    "                continue\n",
    "                \n",
    "            # Move to device\n",
    "            pixel_values = batch[\"pixel_values\"].to(device, non_blocking=True)\n",
    "            input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "            labels = batch[\"labels\"].to(device, non_blocking=True)\n",
    "\n",
    "            # Forward pass\n",
    "            with autocast():\n",
    "                outputs = model(\n",
    "                    pixel_values=pixel_values,\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                total_loss += outputs.loss.item()\n",
    "                batch_count += 1\n",
    "            \n",
    "            # Memory management\n",
    "            if batch_count % 25 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    model.train()\n",
    "    avg_loss = total_loss / max(batch_count, 1)\n",
    "    \n",
    "    print(f\"Ultra validation complete: {batch_count} batches\")\n",
    "    print(f\"Average validation loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "# === Ultra Training Tracker ===\n",
    "class UltraTracker:\n",
    "    def __init__(self, log_path):\n",
    "        self.log_path = log_path\n",
    "        self.history = []\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_epoch = 0\n",
    "        self.start_time = time.time()\n",
    "        self.improvements = 0\n",
    "        self.learning_curve = []\n",
    "    \n",
    "    def log_epoch(self, epoch, train_loss, val_loss, lr, save_time=None):\n",
    "        entry = {\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'learning_rate': lr,\n",
    "            'elapsed_time': time.time() - self.start_time,\n",
    "            'is_best': val_loss < self.best_val_loss,\n",
    "            'save_time': save_time,\n",
    "            'improvement_ratio': (self.best_val_loss - val_loss) / self.best_val_loss if self.best_val_loss != float('inf') else 0\n",
    "        }\n",
    "        \n",
    "        if val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.best_epoch = epoch + 1\n",
    "            self.improvements += 1\n",
    "            entry['improvement_number'] = self.improvements\n",
    "        \n",
    "        self.history.append(entry)\n",
    "        self.learning_curve.append((epoch + 1, train_loss, val_loss))\n",
    "        \n",
    "        # Save log\n",
    "        self._save_log()\n",
    "    \n",
    "    def _save_log(self):\n",
    "        try:\n",
    "            with open(self.log_path, 'w') as f:\n",
    "                json.dump({\n",
    "                    'training_history': self.history,\n",
    "                    'best_val_loss': self.best_val_loss,\n",
    "                    'best_epoch': self.best_epoch,\n",
    "                    'total_improvements': self.improvements,\n",
    "                    'learning_curve': self.learning_curve\n",
    "                }, f, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save ultra training log: {e}\")\n",
    "    \n",
    "    def is_best_model(self, val_loss):\n",
    "        return val_loss < self.best_val_loss\n",
    "    \n",
    "    def plot_learning_curve(self):\n",
    "        \"\"\"Plot learning curve for analysis.\"\"\"\n",
    "        if len(self.learning_curve) > 1:\n",
    "            epochs, train_losses, val_losses = zip(*self.learning_curve)\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(epochs, train_losses, 'b-', label='Training Loss', alpha=0.8)\n",
    "            plt.plot(epochs, val_losses, 'r-', label='Validation Loss', alpha=0.8)\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Ultra Training Learning Curve')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.savefig(os.path.join(os.path.dirname(config.SAVE_PATH), 'learning_curve.png'), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "# === MAIN ULTRA TRAINING FUNCTION ===\n",
    "def main():\n",
    "    print(\"ULTRA-OPTIMIZED BLIP TRAINING - Maximum Accuracy Focus\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load and analyze data\n",
    "    print(\"Loading and analyzing dataset...\")\n",
    "    df = pd.read_csv(config.TRAIN_CSV_PATH)\n",
    "    df['caption'] = df['caption'].fillna('').astype(str)\n",
    "    \n",
    "    # Remove empty captions\n",
    "    df = df[df['caption'].str.len() > 0].reset_index(drop=True)\n",
    "    print(f\"Total valid samples: {len(df):,}\")\n",
    "    \n",
    "    # Load ultra model\n",
    "    model, processor = load_ultra_model()\n",
    "    \n",
    "    # Smart data split\n",
    "    print(\"Creating optimized train/validation split...\")\n",
    "    train_df, val_df = train_test_split(\n",
    "        df, \n",
    "        test_size=config.VALIDATION_SPLIT,\n",
    "        random_state=config.SEED,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(train_df):,}\")\n",
    "    print(f\"Validation samples: {len(val_df):,}\")\n",
    "    \n",
    "    # Create ultra datasets\n",
    "    print(\"Creating ultra datasets...\")\n",
    "    train_dataset = UltraCaptionDataset(train_df, config.IMAGE_FOLDER, processor, is_training=True)\n",
    "    val_dataset = UltraCaptionDataset(val_df, config.IMAGE_FOLDER, processor, is_training=False)\n",
    "    \n",
    "    # Create optimized data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=ultra_collate_fn,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=ultra_collate_fn,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Training batches: {len(train_loader):,}\")\n",
    "    print(f\"Validation batches: {len(val_loader):,}\")\n",
    "    \n",
    "    # Setup ultra optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.LEARNING_RATE,\n",
    "        weight_decay=config.WEIGHT_DECAY,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8,\n",
    "        amsgrad=True  # More stable variant\n",
    "    )\n",
    "    \n",
    "    # Calculate training steps\n",
    "    steps_per_epoch = len(train_loader) // config.ACCUMULATION_STEPS\n",
    "    total_steps = steps_per_epoch * config.NUM_EPOCHS\n",
    "    warmup_steps = int(config.WARMUP_RATIO * total_steps)\n",
    "    \n",
    "    # Ultra scheduler with cosine restarts\n",
    "    if config.USE_COSINE_RESTART:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, \n",
    "            T_0=steps_per_epoch * config.RESTART_FACTOR,\n",
    "            T_mult=2,\n",
    "            eta_min=config.LEARNING_RATE * 0.01\n",
    "        )\n",
    "    else:\n",
    "        scheduler = get_scheduler(\n",
    "            \"cosine\",\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "    \n",
    "    # Mixed precision scaler\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Ultra tracker\n",
    "    tracker = UltraTracker(config.LOG_PATH)\n",
    "    \n",
    "    # Print ultra configuration\n",
    "    print(\"\\nULTRA TRAINING CONFIGURATION:\")\n",
    "    print(f\"   Epochs: {config.NUM_EPOCHS}\")\n",
    "    print(f\"   Batch size: {config.BATCH_SIZE}\")\n",
    "    print(f\"   Accumulation steps: {config.ACCUMULATION_STEPS}\")\n",
    "    print(f\"   Effective batch size: {config.BATCH_SIZE * config.ACCUMULATION_STEPS}\")\n",
    "    print(f\"   Total steps: {total_steps:,}\")\n",
    "    print(f\"   Warmup steps: {warmup_steps:,}\")\n",
    "    print(f\"   Learning rate: {config.LEARNING_RATE}\")\n",
    "    print(f\"   Weight decay: {config.WEIGHT_DECAY}\")\n",
    "    print(f\"   Early stopping patience: {config.EARLY_STOPPING_PATIENCE}\")\n",
    "    print(f\"   Focal Loss: {config.USE_FOCAL_LOSS}\")\n",
    "    print(f\"   Label Smoothing: {config.USE_LABEL_SMOOTHING}\")\n",
    "    print(f\"   Cosine Restart: {config.USE_COSINE_RESTART}\")\n",
    "    \n",
    "    # === ULTRA TRAINING LOOP ===\n",
    "    print(\"\\nSTARTING ULTRA-OPTIMIZED TRAINING\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    model.train()\n",
    "    patience_counter = 0\n",
    "    step = 0\n",
    "    \n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\nEPOCH {epoch + 1}/{config.NUM_EPOCHS}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        batches_processed = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Ultra training progress\n",
    "        progress_bar = tqdm(\n",
    "            train_loader, \n",
    "            desc=f\"Ultra Epoch {epoch + 1}\",\n",
    "            ncols=120,\n",
    "            postfix={'loss': '0.0000', 'lr': f'{config.LEARNING_RATE:.2e}'}\n",
    "        )\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            if batch is None:\n",
    "                continue\n",
    "                \n",
    "            # Move to device\n",
    "            pixel_values = batch[\"pixel_values\"].to(device, non_blocking=True)\n",
    "            input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "            labels = batch[\"labels\"].to(device, non_blocking=True)\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with autocast():\n",
    "                outputs = model(\n",
    "                    pixel_values=pixel_values,\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = outputs.loss / config.ACCUMULATION_STEPS\n",
    "            \n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            epoch_loss += loss.item() * config.ACCUMULATION_STEPS\n",
    "            batches_processed += 1\n",
    "            \n",
    "            # Gradient accumulation and optimization\n",
    "            if (batch_idx + 1) % config.ACCUMULATION_STEPS == 0:\n",
    "                # Gradient clipping\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.MAX_GRAD_NORM)\n",
    "                \n",
    "                # Optimizer step\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                step += 1\n",
    "                \n",
    "                # Update progress\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{loss.item() * config.ACCUMULATION_STEPS:.4f}',\n",
    "                    'lr': f'{current_lr:.2e}',\n",
    "                    'step': f'{step}/{total_steps}'\n",
    "                })\n",
    "            \n",
    "            # Memory management\n",
    "            if batch_idx % 100 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "        \n",
    "        # === END OF EPOCH EVALUATION ===\n",
    "        epoch_train_time = time.time() - epoch_start_time\n",
    "        print(f\"\\nEPOCH {epoch + 1} COMPLETE\")\n",
    "        print(f\"Training time: {epoch_train_time:.1f}s\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_train_loss = epoch_loss / max(batches_processed, 1)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Ultra validation\n",
    "        print(\"Running ultra validation...\")\n",
    "        val_start_time = time.time()\n",
    "        val_loss = ultra_evaluation(\n",
    "            model, val_loader, device, \n",
    "            f\"Epoch {epoch + 1} Ultra Validation\"\n",
    "        )\n",
    "        val_time = time.time() - val_start_time\n",
    "        \n",
    "        # Model saving logic\n",
    "        save_time = None\n",
    "        if tracker.is_best_model(val_loss):\n",
    "            patience_counter = 0\n",
    "            print(\"NEW ULTRA BEST MODEL!\")\n",
    "            \n",
    "            # Save the ultra model\n",
    "            save_start = time.time()\n",
    "            os.makedirs(config.SAVE_PATH, exist_ok=True)\n",
    "            model.save_pretrained(config.SAVE_PATH)\n",
    "            processor.save_pretrained(config.SAVE_PATH)\n",
    "            save_time = time.time() - save_start\n",
    "            \n",
    "            print(f\"Ultra model saved in {save_time:.2f}s\")\n",
    "            print(f\"Validation loss improved: {tracker.best_val_loss:.6f} â†’ {val_loss:.6f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement. Patience: {patience_counter}/{config.EARLY_STOPPING_PATIENCE}\")\n",
    "        \n",
    "        # Ultra epoch summary\n",
    "        print(f\"\\nEPOCH {epoch + 1} ULTRA SUMMARY:\")\n",
    "        print(f\"   Train Loss: {avg_train_loss:.6f}\")\n",
    "        print(f\"   Val Loss: {val_loss:.6f}\")\n",
    "        print(f\"   Learning Rate: {current_lr:.2e}\")\n",
    "        print(f\"   Train Time: {epoch_train_time:.1f}s\")\n",
    "        print(f\"   Val Time: {val_time:.1f}s\")\n",
    "        if save_time:\n",
    "            print(f\"   Save Time: {save_time:.1f}s\")\n",
    "        print(f\"   Best Val Loss: {tracker.best_val_loss:.6f} (Epoch {tracker.best_epoch})\")\n",
    "        \n",
    "        # Log to ultra tracker\n",
    "        tracker.log_epoch(epoch, avg_train_loss, val_loss, current_lr, save_time)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if patience_counter >= config.EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"\\nULTRA EARLY STOPPING TRIGGERED\")\n",
    "            print(f\"   No improvement for {config.EARLY_STOPPING_PATIENCE} epochs\")\n",
    "            print(f\"   Best ultra model from epoch {tracker.best_epoch}\")\n",
    "            break\n",
    "        \n",
    "        print(\"-\" * 70)\n",
    "    \n",
    "    # === ULTRA TRAINING COMPLETE ===\n",
    "    total_training_time = time.time() - tracker.start_time\n",
    "    \n",
    "    print(\"\\nULTRA TRAINING COMPLETED!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Best validation loss: {tracker.best_val_loss:.6f}\")\n",
    "    print(f\"Best epoch: {tracker.best_epoch}\")\n",
    "    print(f\"Total improvements: {tracker.improvements}\")\n",
    "    print(f\"Final epoch: {epoch + 1}\")\n",
    "    print(f\"Total training time: {total_training_time:.1f}s ({total_training_time/3600:.2f} hours)\")\n",
    "    print(f\"Ultra model saved to: {config.SAVE_PATH}\")\n",
    "    \n",
    "    # Generate learning curve\n",
    "    tracker.plot_learning_curve()\n",
    "    \n",
    "    # Save comprehensive ultra summary\n",
    "    final_summary = {\n",
    "        'ultra_training_config': {\n",
    "            'base_model': config.BASE_MODEL,\n",
    "            'batch_size': config.BATCH_SIZE,\n",
    "            'accumulation_steps': config.ACCUMULATION_STEPS,\n",
    "            'learning_rate': config.LEARNING_RATE,\n",
    "            'weight_decay': config.WEIGHT_DECAY,\n",
    "            'num_epochs': config.NUM_EPOCHS,\n",
    "            'warmup_ratio': config.WARMUP_RATIO,\n",
    "            'early_stopping_patience': config.EARLY_STOPPING_PATIENCE,\n",
    "            'focal_loss': config.USE_FOCAL_LOSS,\n",
    "            'label_smoothing': config.USE_LABEL_SMOOTHING,\n",
    "            'cosine_restart': config.USE_COSINE_RESTART,\n",
    "            'advanced_augmentation': config.USE_ADVANCED_AUGMENTATION,\n",
    "            'seed': config.SEED\n",
    "        },\n",
    "        'ultra_results': {\n",
    "            'best_val_loss': tracker.best_val_loss,\n",
    "            'best_epoch': tracker.best_epoch,\n",
    "            'total_epochs_trained': epoch + 1,\n",
    "            'total_improvements': tracker.improvements,\n",
    "            'total_training_time_seconds': total_training_time,\n",
    "            'total_training_time_hours': total_training_time / 3600,\n",
    "            'final_learning_rate': current_lr,\n",
    "            'total_steps': step,\n",
    "            'improvement_rate': tracker.improvements / (epoch + 1)\n",
    "        },\n",
    "        'model_info': {\n",
    "            'total_parameters': sum(p.numel() for p in model.parameters()),\n",
    "            'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "            'device': str(device),\n",
    "            'gpu_name': torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU'\n",
    "        },\n",
    "        'data_info': {\n",
    "            'total_samples': len(df),\n",
    "            'training_samples': len(train_df),\n",
    "            'validation_samples': len(val_df),\n",
    "            'validation_split': config.VALIDATION_SPLIT,\n",
    "            'training_batches': len(train_loader),\n",
    "            'validation_batches': len(val_loader)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(config.SAVE_PATH, 'ultra_training_summary.json')\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(final_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"Ultra training summary saved to: {summary_path}\")\n",
    "    \n",
    "    # Final cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"\\nULTRA OPTIMIZATION COMPLETE - MAXIMUM ACCURACY MODEL READY!\")\n",
    "    print(\"Your ultra-optimized BLIP model is ready for inference!\")\n",
    "    \n",
    "    return model, processor, tracker\n",
    "\n",
    "# === Ultra Model Inference Function ===\n",
    "def ultra_inference(model, processor, image_path, device, num_beams=5, max_length=50):\n",
    "    \"\"\"Ultra-optimized inference function for maximum quality captions.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    try:\n",
    "        # Load and preprocess image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Process image\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Generate caption with advanced settings\n",
    "            generated_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams,\n",
    "                early_stopping=True,\n",
    "                do_sample=False,  # Deterministic for best quality\n",
    "                temperature=1.0,\n",
    "                repetition_penalty=1.1,\n",
    "                length_penalty=1.0,\n",
    "                no_repeat_ngram_size=2\n",
    "            )\n",
    "        \n",
    "        # Decode caption\n",
    "        caption = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        return caption.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in ultra inference: {e}\")\n",
    "        return \"Error generating caption\"\n",
    "\n",
    "# === Quick Test Function ===\n",
    "def quick_ultra_test():\n",
    "    \"\"\"Quick test to verify ultra model loading and basic functionality.\"\"\"\n",
    "    print(\"Quick Ultra Test\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    try:\n",
    "        # Test model loading\n",
    "        model, processor = load_ultra_model()\n",
    "        print(\"Ultra model loaded successfully\")\n",
    "        \n",
    "        # Test dataset creation\n",
    "        df_sample = pd.DataFrame({\n",
    "            'image_id': ['test1', 'test2'],\n",
    "            'caption': ['a test image', 'another test caption']\n",
    "        })\n",
    "        \n",
    "        test_dataset = UltraCaptionDataset(\n",
    "            df_sample, \n",
    "            config.IMAGE_FOLDER, \n",
    "            processor, \n",
    "            is_training=False\n",
    "        )\n",
    "        print(\"Ultra dataset creation successful\")\n",
    "        \n",
    "        # Test data loading\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=1,\n",
    "            collate_fn=ultra_collate_fn\n",
    "        )\n",
    "        print(\"Ultra data loader creation successful\")\n",
    "        \n",
    "        print(\"All ultra components working correctly!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ultra test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Uncomment to run quick test first\n",
    "    # if quick_ultra_test():\n",
    "    #     print(\"Starting ultra training...\")\n",
    "    #     main()\n",
    "    # else:\n",
    "    #     print(\"Ultra test failed. Please check your setup.\")\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# === Load fine-tuned model and processor ===\n",
    "model_path = \"/content/drive/MyDrive/Obss Data/blip_finetuned_ULTRA\"\n",
    "processor = BlipProcessor.from_pretrained(model_path)\n",
    "model = BlipForConditionalGeneration.from_pretrained(model_path).to(\"cuda\").eval()\n",
    "\n",
    "# === Load test CSV and test images folder ===\n",
    "test_csv = \"/content/drive/MyDrive/Obss Data/test.csv\"  # contains column 'image_id'\n",
    "test_image_folder = \"/content/drive/MyDrive/Obss Data/test/test\"\n",
    "test_df = pd.read_csv(test_csv)\n",
    "\n",
    "results = []\n",
    "\n",
    "# === Inference loop ===\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    image_id = row[\"image_id\"]\n",
    "    image_path = os.path.join(test_image_folder, f\"{image_id}.jpg\")\n",
    "\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=30,\n",
    "                num_beams=5,\n",
    "                do_sample=False\n",
    "            )\n",
    "            caption = processor.decode(output_ids[0], skip_special_tokens=True).strip()\n",
    "    except Exception as e:\n",
    "        caption = f\"[ERROR: {str(e)}]\"\n",
    "\n",
    "    results.append({\n",
    "        \"image_id\": image_id,\n",
    "        \"caption\": caption\n",
    "    })\n",
    "\n",
    "# === Save predictions ===\n",
    "submission_df = pd.DataFrame(results)\n",
    "submission_path = \"/content/drive/MyDrive/Obss Data/submission_blip_finetuned_v11.csv\"\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"Captions saved to {submission_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12268213,
     "sourceId": 101408,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
